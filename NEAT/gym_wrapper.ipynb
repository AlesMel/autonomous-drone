{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BatchedStepResult' from 'mlagents_envs.base_env' (c:\\Users\\alesm\\miniconda3\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\base_env.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlagents_envs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnityEnvironment\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlagents_envs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mside_channel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine_configuration_channel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EngineConfigurationChannel\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlagents_envs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchedStepResult\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUnityGymModifiedException\u001b[39;00m(error\u001b[38;5;241m.\u001b[39mError):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    Any error related to the gym wrapper of ml-agents.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BatchedStepResult' from 'mlagents_envs.base_env' (c:\\Users\\alesm\\miniconda3\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\base_env.py)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gym wrapper for a Unity ML-Agents environment. \n",
    "Currently does not support:\n",
    "-multiple brains in the env\n",
    "-flattened branch\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from mlagents_envs.base_env import BatchedStepResult\n",
    "\n",
    "class UnityGymModifiedException(error.Error):\n",
    "    \"\"\"\n",
    "    Any error related to the gym wrapper of ml-agents.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class UnityEnvModified(gym.Env):\n",
    "    def __init__(self, environment_filename, worker_id=0, multiagent=False, no_graphics=False, width=80, height=80, time_scale=20.0):\n",
    "        \"\"\"\n",
    "        Environment initialization\n",
    "        :param environment_filename: The UnityEnvironment path or file to be wrapped in the gym.\n",
    "        :param worker_id: Worker number for environment. Default 0.\n",
    "        :param multiagent: Whether to run in multi-agent mode (lists of obs, reward, done). Default False.\n",
    "        :param no_graphics: Whether to run the Unity simulator in no-graphics mode. Default False.\n",
    "        :param width: Defines the width of the display. Default 80.0.\n",
    "        :param height: Defines the height of the display. Default 80.0.\n",
    "        :param time_scale: Defines the multiplier for the deltatime in the simulation. If set to a higher value, time will pass faster in the simulation but the physics might break. Default 1.0.\n",
    "        \"\"\"\n",
    "        base_port = 5005\n",
    "        if environment_filename is None:\n",
    "            base_port = UnityEnvironment.DEFAULT_EDITOR_PORT\n",
    "        \n",
    "        channel = EngineConfigurationChannel()\n",
    "        \n",
    "        self._env = UnityEnvironment(environment_filename, worker_id, base_port, no_graphics=no_graphics, side_channels=[channel])\n",
    "        channel.set_configuration_parameters(width=width)\n",
    "        channel.set_configuration_parameters(height=height)\n",
    "        channel.set_configuration_parameters(time_scale=time_scale)\n",
    "        \n",
    "        if not self._env.get_agent_groups():\n",
    "            self._env.step()\n",
    "        \n",
    "        self._multiagent = multiagent\n",
    "        self.brain_name = self._env.get_agent_groups()[0]\n",
    "        self.group_spec = self._env.get_agent_group_spec(self.brain_name)\n",
    "        \n",
    "        self._env.reset()\n",
    "        step_result = self._env.get_step_result(self.brain_name)\n",
    "        \n",
    "        self._previous_step_result = step_result\n",
    "        self._previous_new_id_order = list(range(step_result.n_agents()))\n",
    "        self._previous_done_agents = 0\n",
    "        \n",
    "        self._agents_id = list(self._previous_step_result.agent_id)\n",
    "        \n",
    "        self.n_agents = len(self._agents_id)\n",
    "        self._n_actions_to_send = self.n_agents\n",
    "        \n",
    "        # Check brain configuration\n",
    "        if len(self._env.get_agent_groups()) != 1:\n",
    "            raise UnityGymException(\n",
    "                \"There can only be one brain in a UnityEnvironment \"\n",
    "                \"if it is wrapped in a gym.\"\n",
    "            )\n",
    "        \n",
    "        if self.group_spec.is_action_discrete():\n",
    "            branches = self.group_spec.discrete_action_branches\n",
    "            if self.group_spec.action_shape == 1:\n",
    "                self.action_space = spaces.Discrete(branches[0])\n",
    "            else:\n",
    "                self.action_space = spaces.MultiDiscrete(branches)\n",
    "        else:\n",
    "            high = np.array([1] * self.group_spec.action_shape)\n",
    "            self.action_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "            \n",
    "        high = np.array([np.inf] * self._get_vec_obs_size())\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the state of the environment and returns an initial observation.\n",
    "        In the case of multi-agent environments, this is a list.\n",
    "        Returns: observation (object/list): the initial observation of the\n",
    "            space.\n",
    "        \"\"\"\n",
    "        \n",
    "        step_result = self._step(True)\n",
    "        \n",
    "        if self._multiagent:\n",
    "            return self._multi_step(step_result)[0]\n",
    "        else:\n",
    "            return self._single_step(step_result)[0]\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"Override _close in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        self._env.close()\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. \n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        In the case of multi-agent environments, these are lists.\n",
    "        Args:\n",
    "            action (object/list): an action provided by the environment\n",
    "        Returns:\n",
    "            observation (object/list): agent's observation of the current environment\n",
    "            reward (float/list) : amount of reward returned after previous action\n",
    "            done (boolean/list): whether the episode has ended.\n",
    "            maxstep (boolean/list): whether the episode has ended because the agent ran out of time.\n",
    "        \"\"\"\n",
    "        \n",
    "        action = self._sanitize_action(action)\n",
    "        self._env.set_actions(self.brain_name, action)\n",
    "        step_result = self._step()\n",
    "        \n",
    "        if self._multiagent:\n",
    "            return self._multi_step(step_result)\n",
    "        else:\n",
    "            return self._single_step(step_result)\n",
    "    \n",
    "    def _sanitize_step_result(self, step_result):\n",
    "        \"\"\"\n",
    "        Takes as input a BatchedStepResult returned from mlagents_envs and cleans it in order to send back informations about agents in always the same order.\n",
    "        This order is given by self._agents_id. 2 possible cases :\n",
    "        1) No agents terminated on the new timestep\n",
    "        2) One or more agents aterminated on the new timestep\n",
    "        \n",
    "        If 1), the step_result doesnt need to be modified.\n",
    "        If 2), modifications need to be made on the step_result.\n",
    "        \n",
    "        For some reasons, when an agent is done, mlagents_envs returns in step_result informations about the done agent as well as informations about a new agent,\n",
    "        added because the agent terminated. We want to treat these two agents as the same agent. Furthermore, the information about the new agent is located at a\n",
    "        specific position in the step_result.\n",
    "        To illustrate this, let's say we receive this step_result at timestep t: [0, 1, 2] and agent 1 terminated at t+1. We will receive : [1, 0, 3, 2].\n",
    "        Few things happen here:\n",
    "        -the done agent (1) is put at the first place of the step_result at t+1.\n",
    "        -the new agent (3) is put at the index that agent 1 was on the last timestep, + 1.\n",
    "        \n",
    "        In fact, we can generalize this in the case of n agents being done at timestep t+1: the index of a new agent corresponding to a certain agent which just\n",
    "        terminated is the index of the agent that terminated on the last timestep + n - m, n being the number of done agents at timestep t+1, and m the number of done agents at timestep t.\n",
    "        Why n ? Because n agents were \"pushed\" at the beginning of step_result thus we need to include them to access the new agent.\n",
    "        Why m ? If agents were done at timestep t, they have been removed from the step_result of timestep t+1. We thus need to substract them to access the new agent\n",
    "        (it is easier to see this if you take a pencil and a paper and simulate the process)\n",
    "        \n",
    "        So, in order to return a step_result which is \"sanitized\" i.e. return a step_result with the same order as self._agents_id, we need to do a few things :\n",
    "        -create new_id_order: list of index corresponding to locations of self._agents_id \n",
    "                              (if new_id_order = [2, 0, 1], then id of index 0 in self._agents_id is located at index 2 in step_result, id of index 1 at index 0, and id of index 2 at index 1)\n",
    "        -create index_gym_id_done: list of index of agent ids in self._agents_id that terminated at current timestep (done=True)\n",
    "        -replace agents which are done by their successor agents in self._agents_id and create agents_new_id, a list of the new agents.\n",
    "         To do that, we use the previous step result to locate the position of each done agent. We then deduce the position of their successor (index + n, as said above).\n",
    "         Once we have the position of their successoir, we access their id.\n",
    "        -create new step_result, which is composed of:\n",
    "            -obs: observations of all agents. NOTE: mlagents_envs doesnt provide the last observation (S_T) of a done agent, so we return instead the first observation\n",
    "                  of its successor.\n",
    "            -rewards: rewards obtained by all agents. We return step_result.reward[new_id_order] in order to rank them in the right order.\n",
    "            -dones: whether or not agent termianted on the timestep. We return done=step_result.done[new_id_order] in order to rank them in the right order.\n",
    "            -max_step: whether or not the agent terminated by running out of timesteps. We return step_result.max_step[new_id_order] in order to rank them in the right order.\n",
    "            -agent_id: list of agent ids.\n",
    "            -action_mask: not implemented, so None.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        #Case 1): simply return step_result\n",
    "        # in this case: no done agents, the order of step_result is thus the same as the order of self._agents_id\n",
    "        # so we can set new_id_order to be range(n) ([0, 1, 2, ..., n])\n",
    "        if len(self._agents_id) == step_result.n_agents():\n",
    "            self._previous_step_result = step_result\n",
    "            self._previous_new_id_order = list(range(len(self._agents_id)))\n",
    "            self._previous_done_agents = 0\n",
    "            \n",
    "            return step_result\n",
    "        \n",
    "        #Case 2): modify step_result\n",
    "        \n",
    "        new_id_order = []\n",
    "        for agent_id in self._agents_id:\n",
    "            agent_id_index_step_result = list(step_result.agent_id).index(agent_id)\n",
    "            new_id_order.append(agent_id_index_step_result)\n",
    "        \n",
    "        index_gym_id_done = []\n",
    "        for index, agent_id in enumerate(step_result.agent_id):\n",
    "            if step_result.done[index]:\n",
    "                index_gym_id_done.append(self._agents_id.index(agent_id))\n",
    "            \n",
    "        agents_new_id = []\n",
    "        #2 things here : -replace in self._agents_id the ids of dones agents by ids of their successor.\n",
    "        #                -create agents_new_id, a list of the successors' ids.\n",
    "        for index_id_done in index_gym_id_done:\n",
    "            index_new_agent = self._previous_new_id_order[index_id_done] + len(index_gym_id_done) - self._previous_done_agents\n",
    "            self._agents_id[index_id_done] = list(step_result.agent_id)[index_new_agent]\n",
    "            agents_new_id.append(list(step_result.agent_id)[index_new_agent])\n",
    "        \n",
    "        new_obs = []\n",
    "        for index, agent_id in enumerate(self._agents_id):\n",
    "            if agent_id in agents_new_id:\n",
    "                new_obs.append(step_result.obs[0][self._previous_new_id_order[index_gym_id_done[agents_new_id.index(agent_id)]] + len(index_gym_id_done) - self._previous_done_agents])\n",
    "            else:\n",
    "                new_obs.append(step_result.obs[0][new_id_order[index]])\n",
    "        new_obs = [np.array(new_obs)]\n",
    "            \n",
    "        self._previous_step_result = step_result\n",
    "        self._previous_new_id_order = new_id_order\n",
    "        self._previous_done_agents = len(index_gym_id_done)\n",
    "        \n",
    "        new_step_result = BatchedStepResult(obs=new_obs, reward=step_result.reward[new_id_order], done=step_result.done[new_id_order], \n",
    "                                            max_step=step_result.max_step[new_id_order], agent_id=step_result.agent_id[new_id_order], action_mask=None)\n",
    "        \n",
    "        return new_step_result\n",
    "    \n",
    "    def _sanitize_action(self, action):\n",
    "        if self._n_actions_to_send > len(self._agents_id):\n",
    "            [action.insert(0, 0) for i in range(self._n_actions_to_send - len(action))]\n",
    "            return np.array(action).reshape((self._n_actions_to_send, self.group_spec.action_size))\n",
    "        else:\n",
    "            return np.array(action).reshape((len(self._agents_id), self.group_spec.action_size))\n",
    "    \n",
    "    def _step(self, reset=False):\n",
    "        if reset:\n",
    "            self._env.reset()\n",
    "        else:\n",
    "            self._env.step()\n",
    "            \n",
    "        step_result = self._env.get_step_result(self.brain_name)\n",
    "        \n",
    "        self._n_actions_to_send = step_result.n_agents()\n",
    "        \n",
    "        while step_result.n_agents() - sum(step_result.done) < self.n_agents:\n",
    "            self._env.step()\n",
    "            step_result_bis = self._env.get_step_result(self.brain_name)\n",
    "        \n",
    "            step_result.obs[0] = np.append(step_result.obs[0], step_result_bis.obs[0], axis=0)\n",
    "            step_result.reward = np.append(step_result.reward, step_result_bis.reward)\n",
    "            step_result.done = np.append(step_result.done, step_result_bis.done)\n",
    "            step_result.max_step = np.append(step_result.max_step, step_result_bis.max_step)\n",
    "            step_result.agent_id = np.append(step_result.agent_id, step_result_bis.agent_id)\n",
    "            step_result.action_mask = np.append(step_result.action_mask, step_result_bis.action_mask)\n",
    "            \n",
    "            self._n_actions_to_send = step_result_bis.n_agents()\n",
    "        \n",
    "        return self._sanitize_step_result(step_result)\n",
    "    \n",
    "    def _single_step(self, step_result):\n",
    "        obs = step_result.obs[0]\n",
    "        if len(obs.shape) == 2:\n",
    "            obs = np.concatenate(obs)\n",
    "        return (obs, step_result.reward[0], step_result.done[0], step_result.max_step[0])\n",
    "    \n",
    "    def _multi_step(self, step_result):\n",
    "        obs = step_result.obs[0]\n",
    "        if len(obs.shape) == 2:\n",
    "            result = []\n",
    "            for obs_agent in obs:\n",
    "                result.append(obs_agent)\n",
    "            obs = result\n",
    "        return (obs, list(step_result.reward), list(step_result.done), list(step_result.max_step))\n",
    "    \n",
    "    def _get_vec_obs_size(self) -> int:\n",
    "        result = 0\n",
    "        for shape in self.group_spec.observation_shapes:\n",
    "            if len(shape) == 1:\n",
    "                result += shape[0]\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
