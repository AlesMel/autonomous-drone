{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alesm\\miniconda3\\envs\\mlagents\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment  # Import Unity environment\n",
    "from mlagents_envs.envs.unity_aec_env import UnityAECEnv\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "from comunication_channel import AgentLogChannel\n",
    "import neat\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 2500\n",
    "NUM_RUNS = 5\n",
    "MAX_GENS = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS: 0\n",
      "DS: 1\n",
      "RotorControl?team=0?agent_id=0\n",
      "RotorControl?team=0?agent_id=1\n",
      "DS: 0\n",
      "DS: 1\n",
      "RotorControl?team=0?agent_id=0\n",
      "RotorControl?team=0?agent_id=1\n",
      "There is total of 4 actions in enviroment\n",
      "There is total of 14 inputs in enviroment\n"
     ]
    }
   ],
   "source": [
    "engine_config_channel = EngineConfigurationChannel()\n",
    "engine_config_channel.set_configuration_parameters(time_scale=1)\n",
    "agent_count_channel = AgentLogChannel()\n",
    "\n",
    "env_path = \"./Builds/train-env/autonomous-drone.exe\"\n",
    "save_nn_destination = 'result/best.pkl'\n",
    "\n",
    "env = UnityEnvironment(file_name=None, worker_id=0, no_graphics=False, side_channels=[engine_config_channel, agent_count_channel])\n",
    "env = UnityParallelEnv(env)\n",
    "env.reset()\n",
    "num_agents = len(env.possible_agents)\n",
    "\n",
    "num_actions = env.action_space(env.possible_agents[0]).shape[0]\n",
    "print(f\"There is total of {num_actions} actions in enviroment\")\n",
    "num_inputs = env.observation_space(env.possible_agents[0]).shape[0]\n",
    "print(f\"There is total of {num_inputs} inputs in enviroment\")\n",
    "\n",
    "MAX_STEPS = 1200\n",
    "NUM_TRIES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_agents_and_double_reset(num_agents: int):\n",
    "    agent_count_channel.send_int(data=num_agents) \n",
    "    env.reset()\n",
    "    obs = env.reset()\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observation_for_agent(agent: int, observations):\n",
    "    for observation in observations:\n",
    "        key = int(observation.split(\"=\")[2])\n",
    "        if key == agent:\n",
    "            return observations[observation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending: 2\n",
      "DS: 0\n",
      "DS: 1\n",
      "RotorControl?team=0?agent_id=0\n",
      "RotorControl?team=0?agent_id=1\n",
      "DS: 0\n",
      "DS: 1\n",
      "RotorControl?team=0?agent_id=0\n",
      "RotorControl?team=0?agent_id=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  3.030908  ,  0.        ,  3.030908  ,  0.        ,\n",
       "        0.        ,  1.        ,  0.        ,  0.        , -0.09739784,\n",
       "        0.        ,  0.        , -0.19296518,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = set_agents_and_double_reset(2)\n",
    "get_observation_for_agent(1, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_agents():\n",
    "    map = {}\n",
    "    current_index = 0\n",
    "    for agent in env.agents:\n",
    "        map[int(agent.split(\"=\")[2])] = current_index\n",
    "        current_index += 1\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map = map_agents()\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS: 0\n",
      "DS: 1\n",
      "RotorControl?team=0?agent_id=0\n",
      "RotorControl?team=0?agent_id=1\n",
      "Sending: 4\n",
      "DS: 0\n",
      "DS: 1\n",
      "DS: 2\n",
      "DS: 3\n",
      "RotorControl?team=0?agent_id=0\n",
      "RotorControl?team=0?agent_id=1\n",
      "RotorControl?team=0?agent_id=2\n",
      "RotorControl?team=0?agent_id=3\n",
      "DS: 0\n",
      "DS: 1\n",
      "DS: 2\n",
      "DS: 3\n",
      "RotorControl?team=0?agent_id=0\n",
      "RotorControl?team=0?agent_id=1\n",
      "RotorControl?team=0?agent_id=2\n",
      "RotorControl?team=0?agent_id=3\n",
      "DS: 0\n",
      "DS: 1\n",
      "DS: 2\n",
      "DS: 3\n",
      "TS: 1\n",
      "RotorControl?team=0?agent_id=0\n",
      "RotorControl?team=0?agent_id=2\n",
      "RotorControl?team=0?agent_id=3\n",
      "Done: {'RotorControl?team=0?agent_id=1': False, 'RotorControl?team=0?agent_id=0': False, 'RotorControl?team=0?agent_id=2': False, 'RotorControl?team=0?agent_id=3': False}\n",
      "Env agents: ['RotorControl?team=0?agent_id=0', 'RotorControl?team=0?agent_id=1', 'RotorControl?team=0?agent_id=1', 'RotorControl?team=0?agent_id=2', 'RotorControl?team=0?agent_id=3']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m env\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m     12\u001b[0m     actions \u001b[38;5;241m=\u001b[39m {agent: random\u001b[38;5;241m.\u001b[39mchoice(possible_actions) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39magents} \u001b[38;5;66;03m# env.action_space(agent).sample()\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     obs, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdone\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnv agents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39magents\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alesm\\miniconda3\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\envs\\unity_parallel_env.py:47\u001b[0m, in \u001b[0;36mUnityParallelEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Step environment\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Agent cleanup and sorting\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cleanup_agents()\n",
      "File \u001b[1;32mc:\\Users\\alesm\\miniconda3\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\envs\\unity_pettingzoo_base_env.py:190\u001b[0m, in \u001b[0;36mUnityPettingzooBaseEnv._step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_states()\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m behavior_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mbehavior_specs\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 190\u001b[0m     dones, rewards, cumulative_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbehavior_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dones\u001b[38;5;241m.\u001b[39mupdate(dones)\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards\u001b[38;5;241m.\u001b[39mupdate(rewards)\n",
      "File \u001b[1;32mc:\\Users\\alesm\\miniconda3\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\envs\\unity_pettingzoo_base_env.py:256\u001b[0m, in \u001b[0;36mUnityPettingzooBaseEnv._batch_update\u001b[1;34m(self, behavior_name)\u001b[0m\n\u001b[0;32m    254\u001b[0m current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mget_steps(behavior_name)\n\u001b[0;32m    255\u001b[0m ds, ts \u001b[38;5;241m=\u001b[39m current_batch\n\u001b[1;32m--> 256\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    257\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mget_steps(behavior_name)\n\u001b[0;32m    258\u001b[0m     ds, ts \u001b[38;5;241m=\u001b[39m current_batch\n",
      "File \u001b[1;32mc:\\Users\\alesm\\miniconda3\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\base_env.py:109\u001b[0m, in \u001b[0;36mDecisionSteps.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "env.reset()\n",
    "target_agents = [4, 4, 4]\n",
    "possible_actions = [[1, 1, 1, 1]]\n",
    "for target in target_agents:\n",
    "    obs = set_agents_and_double_reset(num_agents = target)\n",
    "    assert env.num_agents == target, f\"Target agents do not match num_agents! expected {target} got {env.num_agents}\"\n",
    "    rewards = [0] * target\n",
    "    map = map_agents()\n",
    "    count_done = [False] * target\n",
    "    while env.agents:\n",
    "        actions = {agent: random.choice(possible_actions) for agent in env.agents} # env.action_space(agent).sample()\n",
    "        obs, reward, done, _ = env.step(actions)\n",
    "        print(f\"Done: {done}\")\n",
    "        print(f\"Env agents: {env.agents}\")\n",
    "        for agent in env.agents:\n",
    "            agent_id = int(agent.split(\"=\")[2])\n",
    "            rewards[map[agent_id]] += reward[agent]\n",
    "    print(max(rewards))\n",
    "    print(\"\\nFinished generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "env_helpers, unwrap_batch_steps reutrns only one agent in decision id, and termination_id after it has finished,\n",
    "therefore it end's not when all agents are done but when first agent is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
